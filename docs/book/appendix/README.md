# Приложение: Справочники

## Зачем это нужно?

Этот раздел содержит справочную информацию: глоссарий терминов, чек-листы, шаблоны SOP и таблицы решений. Используйте его как справочник при работе над проектами.

## Глоссарий

**Agent (Агент)** — система, использующая LLM для восприятия, принятия решений и выполнения действий.

**Chain-of-Thought (CoT)** — техника промптинга, заставляющая модель генерировать промежуточные рассуждения.

**Context Window (Контекстное окно)** — максимальное количество токенов, которые модель может обработать за один запрос.

**Eval (Evaluation)** — тест для проверки качества работы агента.

**Few-Shot Learning** — техника промптинга с примерами в контексте.

**Function Calling** — механизм вызова инструментов LLM через структурированный JSON.

**Grounding (Заземление)** — привязка агента к реальным данным через Tools/RAG, чтобы избежать галлюцинаций.

**Human-in-the-Loop (HITL)** — механизм подтверждения критических действий человеком.

**In-Context Learning (ICL)** — способность модели обучаться на примерах внутри промпта.

**Multi-Agent System (MAS)** — система из нескольких агентов, работающих вместе.

**Prompt Injection** — атака на агента через манипуляцию входными данными.

**RAG (Retrieval Augmented Generation)** — техника дополнения контекста агента релевантными документами из базы знаний.

**ReAct (Reason + Act)** — архитектура агента: Thought → Action → Observation → Loop.

**Reflexion** — техника самокоррекции агента через анализ ошибок.

**SOP (Standard Operating Procedure)** — алгоритм действий, закодированный в промпте.

**Temperature** — параметр энтропии распределения вероятностей токенов.

**Token** — единица текста, обрабатываемая моделью (~0.75 слова).

**Tool (Инструмент)** — функция, которую может вызвать агент для взаимодействия с внешним миром.

**Zero-Shot Learning** — техника промптинга без примеров.

## Чек-листы

### Чек-лист: Настройка модели для агента

- [ ] Модель поддерживает Function Calling (проверено через Lab 00)
- [ ] `Temperature = 0` установлена
- [ ] Контекстное окно достаточно большое (минимум 4k токенов)
- [ ] System Prompt запрещает галлюцинации
- [ ] История диалога управляется (обрезается при переполнении)

### Чек-лист: Создание System Prompt

- [ ] Роль (Persona) четко определена
- [ ] Цель (Goal) конкретна и измерима
- [ ] Ограничения (Constraints) явно указаны
- [ ] Формат ответа (Format) описан
- [ ] SOP (если применимо) детально расписан
- [ ] CoT включен для сложных задач
- [ ] Few-Shot примеры добавлены (если нужно)

## Capability Benchmark (Characterization)

Перед тем как строить агентов, необходимо **научно подтвердить**, что выбранная модель обладает необходимыми способностями. В инженерии это называется **Characterization** (характеризация).

### Зачем это нужно?

Мы не верим этикеткам ("Super-Pro-Max Model"). Мы верим тестам.

**Проблема без проверки:** Вы скачали модель "Llama-3-8B-Instruct" и начали строить агента. Через час работы обнаружили, что модель не вызывает инструменты, а только пишет текст. Вы потратили время на отладку кода, хотя проблема была в модели.

**Решение:** Запустите capability benchmark **перед** началом работы. Это сэкономит часы.

### Что мы проверяем?

#### 1. Basic Sanity (Базовая работоспособность)
- Модель отвечает на запросы
- Нет критических ошибок API
- Базовая связность ответов

#### 2. Instruction Following (Следование инструкциям)
- Модель может жестко придерживаться ограничений
- Важно для агентов: они должны возвращать строго определенные форматы
- **Тест:** "Напиши стих, но не используй букву 'а'"
- **Зачем:** Агент должен возвращать строго определенные форматы, а не "размышления"

#### 3. JSON Generation (Генерация JSON)
- Модель может генерировать валидный синтаксис
- Все взаимодействие с инструментами строится на JSON
- Если модель забывает закрыть скобку `}`, агент падает
- **Тест:** "Верни JSON с полями name и age"

#### 4. Function Calling (Использование инструментов)
- Специфический навык модели распознавать определение функций и формировать специальный токен вызова
- Без этого невозможны инструменты (см. [Главу 04: Инструменты](../04-tools-and-function-calling/README.md))
- **Зачем:** Это основа для Lab 02 и всех последующих лаб

### Почему не все модели умеют Tools?

LLM (Large Language Model) — это вероятностный генератор текста. Она не "знает" про функции из коробки.

Механизм **Function Calling** — это результат специальной тренировки (Fine-Tuning). Разработчики модели добавляют в обучающую выборку тысячи примеров вида:

```
User: "Check weather"
Assistant: <special_token>call_tool{"name": "weather"}<end_token>
```

Если вы скачали "голую" Llama 3 (Base model), она не видела этих примеров. Она просто продолжит диалог текстом.

**Как проверить:** Запустите Lab 00 перед началом работы с инструментами.

### Почему `Temperature = 0` критично для агентов?

Температура регулирует "случайность" выбора следующего токена:
- **High Temp (0.8+):** Модель выбирает менее вероятные слова. Хорошо для стихов, творческих задач.
- **Low Temp (0):** Модель всегда выбирает самое вероятное слово (ArgMax). Максимальная детерминированность.

Для агентов, которые должны выдавать строгий JSON или вызов функции, нужна максимальная детерминированность. Любая "творческая" ошибка в JSON сломает парсер.

**Правило:** Для всех агентов устанавливайте `Temperature = 0`.

### Как интерпретировать результаты?

#### ✅ Все тесты прошли
Модель готова для курса. Можно продолжать работу.

#### ⚠️ 3 из 4 тестов прошли
Можно продолжать, но с осторожностью. Возможны проблемы в краевых случаях.

#### ❌ Function Calling провален
**Критично:** Модель не подходит для Lab 02-08. Нужна другая модель.

**Что делать:**
1. Скачайте модель с поддержкой tools:
   - `Hermes-2-Pro-Llama-3-8B`
   - `Mistral-7B-Instruct-v0.2`
   - `Llama-3-8B-Instruct` (некоторые версии)
   - `Gorilla OpenFunctions`
2. Перезапустите тесты

#### ❌ JSON Generation провален
Модель генерирует сломанный JSON (пропущенные скобки, кавычки).

**Что делать:**
1. Попробуйте другую модель
2. Или используйте `Temperature = 0` (но это не всегда помогает)

### Связь с Evals

Capability Benchmark — это примитивный **Eval** (Evaluation). В промышленных системах (LangSmith, PromptFoo) таких тестов сотни.

**Развитие темы:** См. [Главу 09: Evals и Надежность](../09-evals-and-reliability/README.md) для понимания, как строить комплексные evals для проверки качества работы агента.

### Практика

Для выполнения capability benchmark см. [Lab 00: Model Capability Benchmark](../../../labs/lab00-capability-check/README.md).

## Шаблоны SOP

### SOP для инцидента (DevOps)

```
SOP для падения сервиса:
1. Check Status: Проверь HTTP код ответа
2. Check Logs: Если 500/502 — читай последние 20 строк логов
3. Analyze: Найди ключевые слова:
   - "Syntax error" → Rollback
   - "Connection refused" → Check Database
   - "Out of memory" → Restart
4. Action: Примени фикс согласно анализу
5. Verify: Проверь HTTP статус снова
```

### SOP для обработки тикета (Support)

```
SOP для обработки тикета:
1. Read: Прочитай тикет полностью
2. Context: Собери контекст (версия, ОС, браузер)
3. Search: Поищи в базе знаний похожие случаи
4. Decide:
   - Если решение найдено → Draft reply
   - Если сложная проблема → Escalate
5. Respond: Отправь ответ пользователю
```

## Таблицы решений

### Таблица решений для инцидента

| Симптом | Гипотеза | Проверка | Действие | Верификация |
|---------|----------|----------|----------|-------------|
| HTTP 502 | Сервис упал | `check_http()` → 502 | - | - |
| HTTP 502 | Ошибка в логах | `read_logs()` → "Syntax error" | `rollback_deploy()` | `check_http()` → 200 |
| HTTP 502 | Ошибка в логах | `read_logs()` → "Connection refused" | `restart_service()` | `check_http()` → 200 |

## Мини-упражнения

### Упражнение 1: Создайте свой SOP

Создайте SOP для вашего домена по образцу из раздела "Шаблоны SOP":

```text
SOP для [ваша задача]:
1. [Шаг 1]
2. [Шаг 2]
3. [Шаг 3]
```

**Ожидаемый результат:**
- SOP четко описывает процесс действий
- Шаги последовательны и логичны
- Включены проверки и верификация

### Упражнение 2: Создайте таблицу решений

Создайте таблицу решений для вашей задачи по образцу из раздела "Таблицы решений":

| Симптом | Гипотеза | Проверка | Действие | Верификация |
|---------|----------|----------|----------|-------------|
| ...     | ...      | ...      | ...      | ...         |

**Ожидаемый результат:**
- Таблица покрывает основные сценарии
- Для каждого симптома есть гипотеза, проверка, действие и верификация

## Связь с другими главами

- **Промптинг:** Как использовать SOP в промптах, см. [Главу 02: Промптинг](../02-prompt-engineering/README.md)
- **Кейсы:** Примеры использования SOP в реальных агентах, см. [Главу 10: Кейсы](../10-case-studies/README.md)

---

**Навигация:** [← Best Practices](../11-best-practices/README.md) | [Оглавление](../README.md)

