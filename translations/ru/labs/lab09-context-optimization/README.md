# Lab 09: Context Optimization (Оптимизация контекста)

## Цель
Научиться управлять контекстным окном LLM: подсчитывать токены, применять техники оптимизации (обрезка, саммаризация) и реализовать адаптивное управление контекстом.

## Теория

### Проблема переполнения контекста

Когда агент работает в длинном диалоге или выполняет много шагов в автономном цикле, история сообщений растет. Рано или поздно она не влезает в контекстное окно модели (например, 4k токенов для GPT-3.5-turbo).

**Что происходит:**
- Модель не видит начало разговора
- Модель "забывает" важные детали
- API возвращает ошибку "context length exceeded"

### Техники оптимизации

1. **Подсчет токенов** — всегда знать, сколько токенов используется
2. **Обрезка (Truncation)** — оставляем только последние N сообщений
3. **Саммаризация (Summarization)** — сжимаем старые сообщения через LLM
4. **Адаптивное управление** — выбираем технику в зависимости от заполненности
5. **Артефакты (Artifacts)** — большие результаты инструментов храним вне `messages[]`, в контекст кладём только excerpt + `artifact_id`

### Продвинутые техники (из книги)

В главе 13 рассмотрены дополнительные стратегии:

- **Token Counting** — точный подсчёт токенов через tiktoken (production) или приблизительный по словам (1 токен ≈ 0.75 слов для EN, ≈ 0.5 слов для RU)
- **truncateToTokenLimit** — обрезка контекста с сохранением System Prompt и последнего запроса пользователя
- **Стратегии сжатия** — Semantic Compression (сохраняем смысл, убираем воду), Key-Value Extraction (факты в формате "ключ: значение"), стандартная саммаризация
- **Incremental Summarization** — обновление существующего саммари новыми сообщениями вместо пересаммаризации всей истории. O(n) по токенам вместо O(n^2)

Подробнее: [Глава 13: Context Engineering](../../book/13-context-engineering/README.md)

## Задание

В файле `main.go` реализуйте систему оптимизации контекста для длинного диалога.

### Часть 1: Подсчет токенов

Реализуйте функции:
- `estimateTokens(text string) int` — приблизительная оценка (1 токен ≈ 4 символа)
- `countTokensInMessages(messages []openai.ChatCompletionMessage) int` — подсчет токенов во всей истории

### Часть 2: Обрезка истории

Реализуйте функцию `truncateHistory(messages []openai.ChatCompletionMessage, maxTokens int) []openai.ChatCompletionMessage`:
- Всегда сохраняйте System Prompt
- Оставляйте последние сообщения, пока не достигнете лимита

### Часть 3: Саммаризация

Реализуйте функцию `summarizeMessages(messages []openai.ChatCompletionMessage) string`:
- Используйте LLM для создания краткого резюме старых сообщений
- Сохраняйте важные факты, решения, текущее состояние задачи

### Часть 4: Адаптивное управление

Реализуйте функцию `adaptiveContextManagement(messages []openai.ChatCompletionMessage, maxTokens int) []openai.ChatCompletionMessage`:
- Если контекст < 80% — ничего не делаем
- Если 80-90% — применяем приоритизацию (сохраняем важные сообщения)
- Если > 90% — применяем саммаризацию

### Сценарий тестирования

Запустите длинный диалог (20+ сообщений) и убедитесь, что:
1. Агент продолжает помнить начало разговора (через саммаризацию)
2. Контекст не переполняется
3. Агент корректно отвечает на вопросы о ранних сообщениях

## Пример работы

```
User: Привет! Меня зовут Иван, я работаю DevOps инженером.
Assistant: Привет, Иван! Чем могу помочь?

... (много сообщений) ...

User: Помнишь, как меня зовут?
Assistant: Да, конечно! Вас зовут Иван, вы DevOps инженер.
```

**Без оптимизации:** После 20 сообщений контекст переполнится, агент забудет имя.

**С оптимизацией:** Старые сообщения сжаты в саммари, но важная информация (имя, роль) сохранена.

## Важно

- Используйте `OPENAI_BASE_URL` для локальных моделей
- Для саммаризации можно использовать ту же модель или более дешевую/быструю
- Тестируйте на реальных длинных диалогах (20+ сообщений)

## Критерии сдачи

✅ **Сдано:**
- Реализован подсчет токенов
- Реализована обрезка истории
- Реализована саммаризация через LLM
- Реализовано адаптивное управление
- Агент помнит начало разговора после оптимизации
- Контекст не переполняется

❌ **Не сдано:**
- Контекст переполняется
- Агент забывает важную информацию после оптимизации
- Саммаризация не работает
- Код не компилируется

---

**Следующий шаг:** После завершения Lab 09 переходите к [Lab 10: Planning и Workflows](../lab10-planning-workflows/README.md) — декомпозиция задач, разрешение зависимостей и сохранение состояния.

