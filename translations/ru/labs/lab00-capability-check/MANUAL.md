# Методическое пособие: Lab 00 — Model Capability Benchmark

## Зачем это нужно?

Перед тем как строить сложных агентов, мы должны **научно подтвердить**, что наша модель (особенно локальная) обладает необходимыми способностями. В инженерии это называется **Characterization** (характеризация).

Мы не верим этикеткам ("Super-Pro-Max Model"). Мы верим тестам.

### Реальный кейс

**Ситуация:** Вы скачали модель "Llama-3-8B-Instruct" и начали строить агента. Через час работы обнаружили, что модель не вызывает инструменты, а только пишет текст.

**Проблема:** Вы потратили время на отладку кода, хотя проблема была в модели.

**Решение:** Запустите Lab 00 **перед** началом работы. Это сэкономит часы.

## Теория простыми словами

### Что мы проверяем?

1. **Basic Sanity (Базовая работоспособность)**
   - Модель отвечает на запросы
   - Нет критических ошибок API

2. **Instruction Following (Следование инструкциям)**
   - Модель может жестко придерживаться ограничений
   - Важно для агентов: они должны возвращать строго определенные форматы

3. **JSON Generation (Генерация JSON)**
   - Модель может генерировать валидный синтаксис
   - Все взаимодействие с инструментами строится на JSON

4. **Function Calling (Использование инструментов)**
   - Специфический навык модели распознавать определение функций
   - Без этого невозможны Lab 02 и дальше

### Почему не все модели умеют Tools?

LLM (Large Language Model) — это вероятностный генератор текста. Она не "знает" про функции.

Механизм **Function Calling** — это результат специальной тренировки (Fine-Tuning). Разработчики модели добавляют в обучающую выборку тысячи примеров вида:

```
User: "Check weather"
Assistant: <special_token>call_tool{"name": "weather"}<end_token>
```

Если вы скачали "голую" Llama 3 (Base model), она не видела этих примеров. Она просто продолжит диалог текстом.

## Алгоритм выполнения

### Шаг 1: Запуск тестов

```bash
cd labs/lab00-capability-check
export OPENAI_BASE_URL="http://localhost:1234/v1"
export OPENAI_API_KEY="lm-studio"
go run main.go
```

### Шаг 2: Анализ результатов

Тесты выдадут отчет:

```
✅ 1. Basic Sanity - PASSED
✅ 2. Instruction Following - PASSED
✅ 3. JSON Generation - PASSED
❌ 4. Function Calling - FAILED
```

### Шаг 3: Интерпретация

- **Если все тесты прошли:** Модель готова для курса. Можно продолжать.
- **Если Function Calling провален:** Модель не подходит для Lab 02-08. Нужна другая модель.

## Типовые ошибки

### Ошибка 1: "API Error: connection refused"

**Причина:** Локальный сервер (LM Studio/Ollama) не запущен.

**Решение:**
1. Запустите LM Studio
2. Нажмите "Start Server" (обычно порт 1234)
3. Проверьте, что `OPENAI_BASE_URL` указывает на правильный порт

### Ошибка 2: "Function Calling - FAILED"

**Причина:** Модель не обучена на Function Calling.

**Решение:**
1. Скачайте модель с поддержкой tools:
   - `Hermes-2-Pro-Llama-3-8B`
   - `Mistral-7B-Instruct-v0.2`
   - `Llama-3-8B-Instruct` (некоторые версии)
2. Перезапустите тесты

### Ошибка 3: "JSON Generation - FAILED"

**Причина:** Модель генерирует сломанный JSON (пропущенные скобки, кавычки).

**Решение:**
1. Попробуйте другую модель
2. Или используйте `Temperature = 0` (но это не всегда помогает)

## Мини-упражнения

### Упражнение 1: Добавьте свой тест

Добавьте тест на проверку "модель не должна использовать запрещенные слова":

```go
runTest(ctx, client, "5. Safety Check",
    "Say 'Hello' but do NOT use the word 'hi'",
    func(response string) bool {
        return !strings.Contains(strings.ToLower(response), "hi")
    },
)
```

### Упражнение 2: Измерьте latency

Добавьте измерение времени ответа:

```go
start := time.Now()
resp, err := client.CreateChatCompletion(...)
latency := time.Since(start)
fmt.Printf("Latency: %v\n", latency)
```

## Критерии сдачи

✅ **Сдано:** Все 4 теста прошли успешно  
⚠️ **Частично:** 3 из 4 тестов прошли (можно продолжать, но с осторожностью)  
❌ **Не сдано:** Function Calling провален (нужна другая модель)

---

**Следующий шаг:** После успешного прохождения Lab 00 переходите к [Lab 01: Basics](../lab01-basics/README.md)

