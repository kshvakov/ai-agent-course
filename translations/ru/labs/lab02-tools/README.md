# Lab 02: Function Calling (Tools)

## Цель
Понять механизм Function Calling. Научиться описывать функции Go так, чтобы LLM могла их вызывать.

## Важно для локальных моделей ⚠️
Не все локальные модели поддерживают **Function Calling**.
Если вы используете **LM Studio** или **Ollama**, выбирайте модели с тегами `function-calling`, `tool-use` или `agent`.
Хорошие варианты:
*   `Mistral 7B Instruct`
*   `Hermes 2 Pro`
*   `Llama 3 (некоторые тюны)`
*   `Gorilla OpenFunctions`

Если модель не поддерживает тулы, она может просто продолжать разговор текстом, игнорируя ваши инструкции `Tools`.

## Теория
Обычная LLM возвращает текст. Но если описать ей "Инструменты" (Tools) в формате JSON Schema, она может вернуть структурированный запрос на вызов функции.

Процесс:
1.  **Request:** Вы шлете историю + описание функций (Tools Definitions).
2.  **Decision:** Модель решает: "Нужно вызвать функцию".
3.  **Response:** Модель возвращает флаг `ToolCalls` (вместо текста).
4.  **Execution:** Ваш код видит этот флаг и выполняет функцию.

## Задание
У нас есть заглушка функции `GetServerStatus(ip string)`.

1.  **Настройка:** Инициализируйте клиента с `NewClientWithConfig` (как в Lab 01), чтобы работать локально.
2.  **Определение:** Опишите функцию `get_server_status` в `openai.Tool`.
3.  **Запрос:** Отправьте запрос: "Проверь статус сервера 192.168.1.10".
4.  **Обработка:** Проверьте `msg.ToolCalls`. Если не пусто — распечатайте имя функции и аргументы.
